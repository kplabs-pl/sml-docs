Model compilation
=================

Goal
----
In this tutorial you will convert the quantized model into a format that is compatible with the target DPU platform and can leverage the hardware acceleration on the edge.

Prerequisites
-------------
1. `Docker <https://www.docker.com>`_ installed.
2. Model repository and environment set up as described in :ref:`setup_project`.
3. Quantized model created as described in :doc:`/tutorials/ml_deployment/model_deployment/model_quantization`.
4. Target platform fingerprint ``arch.json`` file generated by Vivado. The file is generated by the :ref:`create_bitstream` step of the Vivado process. A sample ``arch.json`` is also provided in ``reference-designs-ml/deployment/arch.json``

Prepare for compilation :tutorial-machine:`Machine Learning Workstation`
------------------------------------------------------------------------
1. Make sure that ``arch.json`` produced by Vivado resides in ``reference-designs-ml/deployment/arch.json``.
2. Enter the Vitis AI deployment container with the working directory volume mounted:

   .. code-block:: shell-session

        customer@ml-workstation:~/reference-designs-ml$ docker run \
            -it \
            -v "$(pwd)":/workspace \
            -e UID="$(id -u)" -e GID="$(id -g)" \
            xilinx/vitis-ai-pytorch-cpu:ubuntu2004-3.5.0.306

Model quantization :tutorial-machine:`Vitis AI Deployment Container`
--------------------------------------------------------------------

Run the following commands in the container environment.

1. Activate the desired conda environment for PyTorch models deployment:

   .. code-block:: shell-session

       vitis-ai-user@vitis-ai-container-id:/workspace$ conda activate vitis-ai-wego-torch2

2. Run the compiler command on the quantized model to produce the FPGA-acceleration-compatible model based on the provided ``arch.json``:

   .. code-block:: shell-session

       (vitis-ai-wego-torch2) vitis-ai-user@vitis-ai-container-id:/workspace$ vai_c_xir \
           --xmodel deployment/deployment_artifacts/quantization_results/Unet_int.xmodel \
           --arch deployment/arch.json \
           --output_dir deployment/deployment_artifacts/compilation_results \
           --net_name deep_globe_segmentation_unet_512_512

   The compiled model should appear in the ``reference-designs-ml/deployment/deployment_artifacts/compilation_results`` directory. If you wish to skip this step it's also available via git-lfs.

   For your convenience the compile command is also provided in the ``reference-designs-ml/deployment/compile_model`` script. You can run it with ``(vitis-ai-wego-torch2) vitis-ai-user@vitis-ai-container-id:/workspace$ ./deployment/compile_model``.

2. Exit the Vitis AI container: ``exit``.
