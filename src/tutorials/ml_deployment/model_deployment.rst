Model deployment
================

Goal
----
In this tutorial you will:
    - Prepare a pre-trained model and data for the deployment process
    - Set up Vitis AI deployment container environment
    - Deploy a pre-trained PyTorch land cover segmentation model to a Vitis AI-compatible format by:
       - quantizing the model to work with efficient numeric representation
       - compiling the model to the format compatible with the given FPGA-based inference accelerator

A bit of background
-------------------
A deep learning model developed with PyTorch or TensorFlow can be converted into a format compatible with the Vitis AI inference accelerator to enable high-performance inference on the edge. This tutorial walks through the process of deploying a pre-trained model to the Vitis AI-compatible format.

The deployment process differs a bit depending on the framework that was used for model training (PyTorch/Tensorflow). This tutorials covers deployment of a PyTorch-based land cover segmentation model for demonstrative purposes.

Prerequisites
-------------
1. `Docker <https://www.docker.com>`_ installed.
2. Model repository and environment set up as described in :ref:`setup_project`.
3. Dataset prepared as described in :ref:`prepare_dataset`.
4. Target platform fingerprint ``arch.json`` file generated by Vivado. The file is generated by the :ref:`create_bitstream` step of the Vivado process, it should reside in the Vivado project path analogous to ``sources_1\bd\dpu_bd\ip\dpu_bd_dpuczdx8g_0_0\arch.json``. If you don't wish to regenerate the architecture a sample ``arch.json`` for ``DPUCZDX8G_ISA1_B1024`` architecture is provided in ``reference-designs-ml/deployment/arch.json``.

.. note:
    The files used in this tutorial are mainly located in the ``deployment`` directory of the `reference-designs-ml` repository.

Prepare the model for deployment :tutorial-machine:`Machine Learning Workstation`
---------------------------------------------------------------------------------

1. Open and run the ``reference-designs-ml/deployment/deployment_preparation.ipynb`` notebook to prepare the quantization calibration subset and extract model weights from the checkpoint.

   The notebook will prepare model weights and a subset of train samples necessary for the quantization step of the deployment. The weights and calibration subset will be saved into ``reference-designs-ml/deployment/deployment_artifacts/deployment_inputs`` directory. Feel free to delve into the notebook and the provided code.

2. Enter the Vitis AI deployment container with the working directory volume mounted:

   .. code-block:: shell-session

        customer@ml-workstation:~/reference-designs-ml$ docker run \
            -it \
            -v "$(pwd)":/workspace \
            -e UID="$(id -u)" -e GID="$(id -g)" \
            xilinx/vitis-ai-pytorch-cpu:ubuntu2004-3.5.0.306


Model quantization and compilation :tutorial-machine:`Vitis AI Deployment Container`
------------------------------------------------------------------------------------

Run the following commands in the container environment.

1. Activate the desired conda environment for PyTorch models deployment:

   .. code-block:: shell-session

       vitis-ai-user@vitis-ai-container-id:/workspace$ conda activate vitis-ai-wego-torch2

2. Install necessary third-party requirements inside the conda environment:

   .. code-block:: shell-session

       (vitis-ai-wego-torch2) vitis-ai-user@vitis-ai-container-id:/workspace$ pip install -r deployment/requirements-vitis-ai.txt


3. Run the quantization script. Feel free to delve into the script to learn more about quantizing PyTorch model for Vitis AI.

   .. code-block:: shell-session

       (vitis-ai-wego-torch2) vitis-ai-user@vitis-ai-container-id:/workspace$ python3 -m deployment.quantize_model

   The quantized model will appear in ``reference-designs-ml/deployment/deployment_artifacts/quantization_results``. If you wish to speed up the process, you can skip this step and use the quantized model provided via git-lfs.

   .. warning::
       Mind that the quantization process is time consuming.

   .. note::
       The quantization process includes evaluation of the quantized model. If you wish to skip this step to speed up the process pass an extra flag that will limit the number of test samples.

       .. code-block:: shell-session

           (vitis-ai-wego-torch2) vitis-ai-user@vitis-ai-container-id:/workspace$ python3 -m deployment.quantize_model --quantization-samples-num-limit 1

4. Run the compiler command on the quantized model to produce the FPGA-acceleration-compatible model based on the provided ``arch.json``:

   .. code-block:: shell-session

       (vitis-ai-wego-torch2) vitis-ai-user@vitis-ai-container-id:/workspace$ vai_c_xir \
           --xmodel deployment/deployment_artifacts/quantization_results/Unet_int.xmodel \
           --arch deployment/arch.json \
           --output_dir deployment/deployment_artifacts/compilation_results \
           --net_name deep_globe_segmentation_unet_512_512

   The compiled model should appear in the ``reference-designs-ml/deployment/deployment_artifacts/compilation_results`` directory. If you wish to skip this step it's also available via git-lfs.

   For your convenience the compile command is also provided in the ``reference-designs-ml/deployment/compile_model`` script. You can run it with ``(vitis-ai-wego-torch2) vitis-ai-user@vitis-ai-container-id:/workspace$ ./deployment/compile_model``.

5. Exit the Vitis AI container: ``exit``.

Evaluate the quantized model metrics :tutorial-machine:`Machine Learning Workstation`
-------------------------------------------------------------------------------------
1. Optionally you can evaluate the quantized model metrics by running the ``reference-designs-ml/deployment/calc_quantized_metrics.ipynb`` notebook.
